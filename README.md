# AI_Assisted_RL_NOTES_1
<br>
🖊Notes🖊<br>

1.Leveraging Skills from Unlabeled Prior Datafor Efficient Online Exploration<br>
<img width="4096" height="2048" alt="0b762ef99d189e6181a10d0e678c3ce" src="https://github.com/user-attachments/assets/dae1d94f-0904-4d7d-8bb9-c5bff0a34282" /><br>
Adopted from [Diversity is All You Need: Learning Skill Embeddings for Hierarchical Reinforcement Learning](https://arxiv.org/pdf/1802.06070)<br>
<img width="1642" height="456" alt="image" src="https://github.com/user-attachments/assets/34bfd04e-f370-4e86-b0a6-b9fb6256aaeb" /><br>

### **3 多样性就是你所需要的一切 (DIVERSITY IS ALL YOU NEED)**

在这项工作中，我们考虑一种无监督的强化学习范式，即智能体被允许进行一个无监督的“探索”阶段，随后再进行一个有监督的阶段。在我们的工作中，无监督阶段的目标是学习技能，以便最终更容易在有监督阶段最大化任务奖励。很方便的是，因为这些技能是在没有先验任务知识的情况下学习的，所以习得的技能可以用于许多不同的任务。

#### **3.1 运作原理**

我们用于无监督技能发现的方法 **DIAYN** (“多样性就是你所需要的一切”) 建立在三个思想之上。首先，为了使技能有用，我们希望技能能决定智能体所访问的状态。不同的技能应该访问不同的状态，因此它们是可区分的。其次，我们希望使用状态而不是动作来区分技能，因为不影响环境的动作对外部观察者是不可见的。例如，如果一个杯子没有移动，外部观察者就无法判断一个机械臂在抓取杯子时施加了多大的力。最后，我们通过学习尽可能随机的技能来鼓励探索，并激励技能尽可能多样化。熵值高但仍可区分的技能必须探索远离其他技能的状态空间部分，否则其动作中的随机性会使它进入无法被区分的状态。

我们使用信息论的符号来构建我们的目标：$S$ 和 $A$ 分别是状态和动作的随机变量；$Z \sim p(z)$ 是一个潜在变量，我们的策略以此为条件；我们将以固定的 $Z$ 为条件的策略称为“技能”；$I(;)$ 和 $H[]$ 分别指互信息和香农熵，两者均以 $e$ 为底计算。在我们的目标中，我们最大化技能和状态之间的互信息 $I(S;Z)$，以体现技能应该控制智能体访问哪些状态的想法。很方便的是，这个互信息表明我们可以从访问过的状态中推断出技能。为了确保使用状态而不是动作来区分技能，我们最小化给定状态下技能和动作之间的互信息 $I(A;Z|S)$。将所有技能与 $p(z)$ 一起看作一个混合策略，我们最大化这个混合策略的熵 $H[A|S]$。

---

总而言之，我们最大化以下表达式：

$F(\theta) = I(S;Z)+H[A|S]-I(A;Z|S)$

$= (H[Z]-H[Z|S])+H[A|S]-(H[A|S]-H[A|S,Z])$

$= H[Z]-H[Z|S]+H[A|S,Z]$

我们重新排列了方程（2）中的目标，以提供直观的优化方法。第一项鼓励我们的先验分布 $p(z)$ 具有高熵。在我们的方法中，我们将 $p(z)$ 固定为均匀分布，从而保证它具有最大熵。第二项表明从当前状态推断出技能 $z$ 应该是容易的。第三项表明每个技能都应该尽可能地随机行动，我们通过使用最大熵策略来表示每个技能来实现这一点。

由于我们无法对所有状态和技能进行积分以精确计算 $p(z|s)$，我们用一个习得的鉴别器 $q_{\phi}(z|s)$ 来近似这个后验概率。詹森不等式（Jensen’s Inequality）告诉我们，用 $q_{\phi}(z|s)$ 替换 $p(z|s)$ 给了我们一个关于目标 $F(\theta)$ 的变分下界 $G(\phi)$（详细推导见 Agakov, 2004）：

$F(\theta) = H[A|S,Z] - H[Z|S] + H[Z]$

$= H[A|S,Z] + E_{z \sim p(z), s \sim \pi(s|z)}[\log p(z|s)] - E_{z \sim p(z)}[\log p(z)]$

$\geq H[A|S,Z] + E_{z \sim p(z), s \sim \pi(s|z)}[\log q_{\phi}(z|s) - \log p(z)] = G(\phi)$

#### **3.2 实现**

我们使用 **软行动者-评论家** (soft actor critic, SAC) 来实现 DIAYN，学习一个以潜在变量 $z$ 为条件的策略 $\pi(a|s,z)$。软行动者-评论家最大化了策略在动作上的熵，这解决了我们目标 $G$ 中的熵项。我们遵循 Haarnoja 等人（2018）的做法，用 $\alpha$ 来缩放熵正则化项 $H[a|s,z]$。我们凭经验发现 $\alpha=0.1$ 在探索和可区分性之间提供了良好的权衡。我们通过用以下伪奖励来替换任务奖励，从而最大化 $G$ 中的期望：

$r_z(s,a) = \log q_{\phi}(z|s) - \log p(z)$

我们对 $p(z)$ 使用一个类别分布。在无监督学习期间，我们在每个情节开始时从 $p(z)$ 中采样一个技能 $z$，并在整个情节中根据该技能进行行动。智能体因访问容易区分的状态而获得奖励，而鉴别器则被更新以更好地从访问过的状态中推断出技能 $z$。熵正则化作为 SAC 更新的一部分发生。

#### **3.3 稳定性**

与之前的对抗性无监督强化学习方法（例如 Sukhbaatar 等人，2017）不同，DIAYN 构成了一个合作博弈，这避免了许多对抗性鞍点公式的不稳定性。在网格世界中，我们可以解析计算出 DIAYN 优化问题的唯一最优解是：将状态在技能之间平均分配，每个技能在其分区上都有一个均匀的平稳分布（证明在附录 B 中）。在连续和近似设置中，收敛保证是可取的，但这是一个非常艰巨的任务：即使是带有函数近似的标准强化学习方法（例如 DQN）也缺乏收敛保证，但这些技术仍然有用。凭经验，我们发现 DIAYN 对随机种子具有鲁棒性；改变随机种子对习得的技能没有明显影响，并且对下游任务的影响也很小（参见图 4、6 和 13）。

我能理解你对这个算法的困惑。**DIAYN (Diversity is All You Need)** 看起来有点抽象，因为它没有直接解决某个具体的强化学习任务，而是专注于一个更基础的问题：**如何有效地探索和学习**。

简单来说，DIAYN 是一种**无监督的技能学习方法**，它的主要作用是作为强化学习的**“预训练”阶段**，为后续的**有监督任务**打下坚实的基础。

---

### DIAYN 对强化学习的作用

你可以把 DIAYN 看作是让一个智能体（比如一个机器人）**“自学成才”** 的过程。在没有外部奖励的情况下，DIAYN 鼓励智能体去探索环境，并自动学习一系列**多样化、有用的“基础技能”**（例如，移动、转圈、跳跃、倒立等等）。

这些基础技能之所以有用，是因为它们：

1.  **提供了有效的探索机制**：传统强化学习在没有奖励的环境中，就像大海捞针一样，效率极低。DIAYN 通过学习可区分的技能，为智能体提供了结构化的探索方式，让它能主动去“尝试”不同的行为，而不是随机乱撞。
2.  **构建了任务无关的“工具箱”**：这些学习到的技能是通用的，不依赖于任何特定任务。它们就像是一个工具箱里的锤子、螺丝刀和钳子，虽然不知道将来会修什么东西，但这些工具本身很有用。

---

### 如何应用到强化学习任务中

将 DIAYN 应用到具体的强化学习任务中，通常分为两个阶段：

#### **第一阶段：无监督的技能预训练**

* **目标**：在没有任务奖励（`reward`）的环境中，运行 DIAYN 算法。
* **过程**：智能体通过最大化自身行为的多样性，自动学习和掌握各种基础技能。比如在一个机器人移动的仿真环境中，智能体可能学习到向前走、向后走、左右转弯、甚至跳跃等技能，而不需要你给出任何奖励。

#### **第二阶段：有监督的任务微调**

* **目标**：解决一个具体的强化学习任务，比如让机器人从A点走到B点。
* **过程**：
    * **方法一：分层强化学习 (Hierarchical RL)**
        在这个阶段，你不再从零开始训练策略，而是把第一阶段学习到的基础技能作为**高层次的动作**。你的策略现在不是选择“向左一步”或“向右一步”，而是选择“执行向前走的技能”或“执行向右转的技能”。这个高层次的策略学习起来会快得多，因为它的“动作空间”更高级、更有效。
    * **方法二：微调 (Fine-Tuning)**
        你也可以直接使用预训练好的策略网络作为初始值，然后用任务特定的奖励（例如，到达目标点的奖励）来继续训练它。由于策略已经有了很好的“行为模式”作为基础，它能更快地收敛到解决任务的最佳方案。

---

### 一个简单的例子

想象一下你要训练一个机器人**学会踢球**。

* **传统方法**：你给它一个奖励函数，当它碰到球就给一点奖励，把球踢进球门就给很多奖励。这就像让一个刚出生的婴儿直接去踢足球，他可能根本不知道怎么移动，更别说去追球了，整个学习过程会非常漫长。
* **使用 DIAYN**：
    1.  **预训练阶段**：你在一个空旷的场地上，让机器人使用 DIAYN 算法进行无监督探索。它自己学会了向前走、向后走、单脚站立、甚至是小跑。
    2.  **任务微调阶段**：现在，你把机器人放到球场上，并给它踢球的奖励。由于它已经掌握了各种基础移动技能，它不再需要从头学习如何走路，而是能把精力集中在如何运用这些技能去靠近球、调整角度、并最终踢进球门。整个学习过程会大大加快。

总而言之，DIAYN 的核心价值在于，它提供了一种**无需奖励的、自我驱动的预训练机制**，这对于解决那些奖励信号稀疏或难以设计的复杂强化学习问题非常有帮助。<br>


2.[Gradient Surgery for Multi-Task Learning](https://proceedings.neurips.cc/paper_files/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf)<br>
[cite_start]这篇论文的标题是《用于多任务学习的梯度手术》（Gradient Surgery for Multi-Task Learning）[cite: 703]。

以下是论文的主要内容概述：

**1. 论文背景与问题**
[cite_start]深度学习和深度强化学习在多任务学习（Multi-task learning, MTL）中面临一个主要挑战：尽管理论上多任务学习可以通过共享结构来提高数据效率，但在实践中，优化问题却常常导致性能不佳，甚至比独立训练任务还要差[cite: 707, 709, 717, 718]。

**2. 核心问题：悲惨三联（The Tragic Triad）**
[cite_start]作者假设，多任务学习中的一个主要优化问题源于不同任务的梯度相互冲突，从而对模型进展产生有害影响[cite: 722][cite_start]。当以下三种情况同时发生时，这种冲突尤为有害[cite: 711, 724, 768]：
* [cite_start]**冲突梯度：** 不同任务的梯度方向相反（余弦相似度为负）[cite: 720, 723, 771]。
* [cite_start]**梯度主导：** 梯度幅度差异很大，导致某个任务的梯度在平均梯度中占据主导地位[cite: 724, 728, 766]。
* [cite_start]**高曲率：** 优化环境中存在高曲率[cite: 724, 727, 767, 768]。

[cite_start]当这三种情况同时出现时，优化器可能会高估主导任务的性能提升，同时低估非主导任务的性能下降，导致难以取得进展[cite: 741, 742, 743]。

**3. 提出的解决方案：PCGrad（Project Conflicting Gradients）**
[cite_start]为了解决这一问题，作者提出了一种名为“梯度手术”（Gradient Surgery）的方法，具体实现为PCGrad（Project Conflicting Gradients，投影冲突梯度）[cite: 712, 746, 747]。

该方法的原理是：
* [cite_start]PCGrad首先检查不同任务梯度之间的余弦相似度[cite: 790, 804]。
* [cite_start]如果两个梯度相互冲突（即余弦相似度为负），PCGrad会将一个任务的梯度投影到另一个任务梯度的法平面上[cite: 712, 790, 805]。
* [cite_start]这个操作会移除梯度中的冲突分量，从而减少有害的梯度干扰[cite: 791]。
* [cite_start]如果梯度没有冲突，则不作修改，允许它们继续进行有益的交互[cite: 788, 802, 806]。

[cite_start]PCGrad是一种与模型架构无关（model-agnostic）的方法，只需对梯度应用进行一次修改，因此可以与现有的多任务学习架构结合使用，以获得更好的性能[cite: 714, 749, 750, 850, 862]。

**4. 实验结果**
[cite_start]作者在多项具有挑战性的多任务监督学习和多任务强化学习问题上评估了PCGrad，并取得了显著的成果[cite: 713, 751, 787]。
* [cite_start]**在多任务监督学习中：** PCGrad在多标签分类和语义分割等任务上表现出色，甚至当与最先进的架构（如路由网络）结合时，性能还能进一步提升，例如在CIFAR-100数据集上，准确率绝对提升了2.8% [cite: 753, 903, 904, 916]。
* [cite_start]**在多任务强化学习中：** PCGrad在Meta-World基准测试上显著优于其他方法，大大提高了数据效率和成功率，例如在MT10和MT50任务上，其性能远超独立训练和多头策略[cite: 752, 576, 577, 578]。
* [cite_start]**消融研究：** 实验证明，PCGrad同时修改梯度方向和幅度的能力对于获得良好性能至关重要[cite: 569, 570, 869]。<br>

3.[Guided Policy Search](https://proceedings.mlr.press/v28/levine13.pdf)<br>
好的，我将详细解释这篇论文的核心思想、具体方法和主要贡献。

### 论文核心思想：融合两种方法的优势

这篇论文的核心思想是**结合两种不同强化学习方法的优点，来解决高维复杂控制问题**。

1.  **第一种方法：直接策略搜索 (Direct Policy Search)**
    * **优点**：可以训练通用的、复杂的策略，比如**神经网络**。它对模型依赖小（model-free），可以直接处理高维状态和动作空间。
    * **缺点**：通常**样本效率非常低**，需要大量的尝试才能找到好的策略。而且，它很容易陷入糟糕的**局部最优解**，导致性能不佳。

2.  **第二种方法：轨迹优化 (Trajectory Optimization)**
    * **优点**：非常**样本高效**，能快速找到一条从起点到终点的最优行为序列（即“轨迹”）。
    * **缺点**：它通常依赖于**精确的系统动力学模型**，而且只能找到一条最优轨迹，无法形成一个通用的、能够适应不同起点的策略。它更像是一个“规划器”，而不是一个通用的“控制器”。

这篇论文提出的“引导式策略搜索”（Guided Policy Search）算法，正是**用高效的轨迹优化来“指导”样本低效但强大的直接策略搜索**。

### 具体方法：分两步走的迭代过程

这个算法是一个迭代过程，每一步都包含两个核心阶段：

**阶段一：轨迹优化（生成“引导样本”）**

* **目标**：利用轨迹优化算法，快速找到一条从特定起点到目标的“好”的动作序列。
* **具体算法**：论文使用了**微分动态规划（DDP）**。
* **如何生成样本**：DDP是一个非常高效的算法，它能根据当前策略和系统的动力学模型，快速计算出一条低成本（高回报）的轨迹。这些轨迹中的每一个状态-动作对（state-action pair），连同DDP计算出的局部成本函数，构成了“引导样本”。
* **作用**：这些样本就像一张**详细的“行动路线图”**，告诉策略学习器哪些动作是好的、应该去执行。

**阶段二：策略学习（训练神经网络）**

* **目标**：训练一个通用的策略（一个神经网络），让它能够**模仿**第一阶段生成的那些“好”的轨迹。
* **具体算法**：作者设计了一种**正则化的重要性采样策略优化**（regularized importance sampled policy optimization）。
* **如何训练**：
    * 它不像传统方法那样需要从头探索和收集样本，而是直接利用第一阶段生成的“引导样本”。
    * “**重要性采样**”技术让算法可以高效地利用这些非当前策略生成的样本来更新策略。
    * “**正则化**”项是关键。它确保学到的策略不会离“引导样本”太远，防止策略学习器偏离正确的方向。这个正则化项就像一个“锚”，将策略拉向DDP找到的有效行为。

**整个算法流程**可以概括为：
1.  **初始化**一个随机策略（神经网络）。
2.  **重复以下步骤**：
    a. **生成引导样本**：根据当前策略，使用DDP等轨迹优化算法，为一系列初始状态生成最优轨迹。
    b. **更新策略**：使用这些轨迹作为训练数据，通过正则化的重要性采样算法来更新神经网络策略的参数。
    c. **重复**，直到策略收敛。

### 主要贡献与优势

1.  **弥合了两种方法的鸿沟**：它成功地将高效的轨迹优化与强大的神经网络策略学习结合在一起，充分利用了两者的优势。
2.  **高样本效率**：通过利用轨迹优化生成的样本，该方法比传统的直接策略搜索方法（如策略梯度）更节省样本，能更快地学习到有效的控制策略。
3.  **避免局部最优解**：轨迹优化过程为策略搜索提供了有效的指导，使其能够避免陷入那些“糟糕”的局部最优解。
4.  **在复杂任务上的成功**：论文在多个高维控制任务（如双足机器人行走、3D人形机器人奔跑）上验证了其有效性，证明了该方法能够成功训练出复杂的神经网络控制器。

4.[Search on the Replay Buffer: Bridging Planning and Reinforcement Learning](https://proceedings.neurips.cc/paper/2019/file/5c48ff18e0a47baaf81d8b8ea51eec92-Paper.pdf)<br>
<img width="1763" height="680" alt="image" src="https://github.com/user-attachments/assets/82a1cc76-e36e-4286-9eb0-df29dfee4e3a" /><br>
这篇论文《在回放缓冲区上搜索：连接规划与强化学习》（Search on the Replay Buffer: Bridging Planning and Reinforcement Learning）旨在解决一个长期存在于强化学习（RL）领域的挑战：在稀疏奖励、高维观察空间和长程任务中，纯粹的强化学习算法往往表现不佳。

论文的核心思想是**将强化学习的优点（能够学习高维环境中的策略）和规划算法的优点（能够进行长程、多步的决策）结合起来**。

以下是该论文的详细解释：

* **面临的问题**：
    * **长程任务**：在需要很多步骤才能到达目标的任务中，比如机器人导航到房间的另一头，传统的强化学习算法很难学到有效策略。
    * **稀疏奖励**：只有在到达最终目标时才有奖励，这使得智能体在训练过程中很少得到正向反馈，很难知道自己是否在朝着正确的方向前进。
    * **高维观察**：例如，直接从图像输入中进行决策，这使得规划变得非常困难，因为状态空间巨大。

* **提出的解决方案：SoRB算法（Search on Replay Buffer）**
    * 论文提出了一种名为SoRB（Search on Replay Buffer）的算法，其核心是把**回放缓冲区（Replay Buffer）**作为**规划的图（Graph）**。
    * **什么是回放缓冲区？** 在强化学习中，回放缓冲区是一个存储智能体过去经验（即状态、动作、奖励、下一个状态）的数据库。
    * **如何构建图？** SoRB算法将回放缓冲区中的每个“状态”（观察）视为图中的一个**节点（node）**。
    * **如何定义边的权重？** 论文使用一个**目标条件下的价值函数**（goal-conditioned value function）来定义任意两个节点（状态）之间的“距离”，这个距离可以被视为图的**边的权重**。
    * **如何规划？** 一旦图构建完成，智能体就可以使用标准的**图搜索算法**（如A*搜索）来寻找从当前状态到目标状态的最短路径。这条路径由一系列中间的**子目标（subgoals）**组成。

* **工作流程**：
    1.  **学习**：首先，智能体使用一个**目标条件下的强化学习算法**（如HER, Hindsight Experience Replay）来学习一个能够从任意状态到达附近任意子目标的策略，并学习一个价值函数来预测状态间的距离。
    2.  **规划**：当智能体需要完成一个远距离目标时，它不再直接尝试，而是首先在回放缓冲区（这个“图”）上进行搜索，找到一条到达最终目标的子目标序列。
    3.  **执行**：然后，智能体依次执行这些子目标。对于每个子目标，它使用在第1步中学到的策略来执行短程动作，直到到达该子目标。

* **主要贡献**：
    * **连接规划与强化学习**：SoRB算法成功地将两种方法的优势结合起来，使得智能体既能处理高维输入（强化学习），又能进行长程规划（图搜索）。
    * **高样本效率**：通过利用回放缓冲区中的历史数据进行规划，算法能够更有效地利用已经收集的经验，减少与环境的交互次数。
    * **解决稀疏奖励问题**：通过将一个大任务分解成一系列更容易解决的子任务，SoRB算法大大简化了长程、稀疏奖励任务的求解。
    * **在图像环境中表现优异**：该方法在图像输入（高维观察）的导航任务中取得了显著成功，证明了其在复杂环境中的泛化能力。
 

5.[Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations](https://arxiv.org/pdf/1709.10087)<br>
<img width="683" height="794" alt="image" src="https://github.com/user-attachments/assets/63c49fd3-1213-4c66-af7b-722483f5ebed" /><br>
好的，这篇论文《利用深度强化学习和演示学习复杂灵巧操作》（Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations）主要研究如何使用深度强化学习（DRL）来控制具有高维度的多指灵巧手，并利用人类演示来提高学习效率。

以下是论文的主要内容概述：

* [cite_start]**面临的挑战**：尽管深度强化学习在控制复杂动态系统方面表现出了潜力，但它在处理高维度的灵巧手操作时面临两大挑战 [cite: 2231, 2232, 2240]：
    1.  [cite_start]**高维和复杂性**：灵巧手具有很高的自由度（例如，论文中使用的手有24个自由度）和大量的接触点，这使得有效的控制变得非常困难 [cite: 2230, 2231]。
    2.  [cite_start]**样本效率低下**：深度强化学习算法通常需要大量的交互数据才能学到有效的策略，这使得在物理机器人上进行训练变得不切实际 [cite: 2232, 2233, 2255]。

* **提出的解决方案**：
    1.  [cite_start]**纯强化学习**：论文首先展示了在模拟环境中，纯粹的、无模型的深度强化学习算法可以从零开始解决复杂的灵巧操作任务，例如物体搬运、手内操作、工具使用和开门 [cite: 2234, 2237, 2260]。
    2.  [cite_start]**融合人类演示**：为了解决样本效率低下的问题，作者提出将**少量的人类演示**数据整合到强化学习训练过程中 [cite: 2235, 2247, 2256]。具体方法是：
        * [cite_start]首先使用行为克隆（behavior cloning）对策略进行**预训练**，使其模仿人类演示的动作 [cite: 2257]。
        * [cite_start]然后，利用策略梯度方法进行**微调**，并增加一个额外的损失函数，以确保策略在微调过程中仍然接近演示数据 [cite: 2257]。

* **主要贡献和发现**：
    * [cite_start]**可扩展性**：论文首次证明了无模型的深度强化学习可以扩展到高维度的灵巧操作任务，解决了以前认为其不具备此能力的局限 [cite: 2234, 2260]。
    * [cite_start]**样本效率大幅提升**：通过结合人类演示，训练所需的样本量大大减少，达到了相当于“几个小时的机器人经验”的水平 [cite: 2235, 2247, 2257, 2262]。
    * [cite_start]**策略质量提升**：使用人类演示训练的策略不仅更具样本效率，而且表现出更自然、更像人类的动作，并且**更具鲁棒性** [cite: 2236, 2257, 2258][cite_start]。这归因于人类演示中包含了有助于学习更稳健策略的先验知识 [cite: 2264]。
    * [cite_start]**任务基准**：为了推动该领域的研究，作者还提出了一系列具有挑战性的灵巧操作任务，这些任务旨在检验真实世界机器人手部所需的高维度和复杂交互能力 [cite: 2251, 2252, 2265]。


